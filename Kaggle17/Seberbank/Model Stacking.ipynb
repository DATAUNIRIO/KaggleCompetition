{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fire Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jiashen/env3/lib/python3.5/site-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv('train_adjusted.csv')\n",
    "test = pd.read_csv('test_c.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert Categorical Into Numerical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "col = list(test.columns)[2:]\n",
    "cat = []\n",
    "for each in col:\n",
    "    if train[each].dtype == 'object' or 'ID' in each:\n",
    "        train[each] = pd.factorize(train[each], sort=True)[0]\n",
    "        test[each]=pd.factorize(test[each], sort=True)[0]\n",
    "        cat.append(each)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleansing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bad_index = train[train.life_sq > train.full_sq].index\n",
    "train.ix[bad_index, \"life_sq\"] = np.NaN\n",
    "bad_index = test[test.life_sq > test.full_sq].index\n",
    "test.ix[bad_index, \"life_sq\"] = np.NaN\n",
    "bad_index = train[train.life_sq < 5].index\n",
    "train.ix[bad_index, \"life_sq\"] = np.NaN\n",
    "bad_index = test[test.life_sq < 5].index\n",
    "test.ix[bad_index, \"life_sq\"] = np.NaN\n",
    "bad_index = train[train.full_sq < 5].index\n",
    "train.ix[bad_index, \"full_sq\"] = np.NaN\n",
    "bad_index = test[test.full_sq < 5].index\n",
    "test.ix[bad_index, \"full_sq\"] = np.NaN\n",
    "bad_index = train[train.kitch_sq >= train.life_sq].index\n",
    "train.ix[bad_index, \"kitch_sq\"] = np.NaN\n",
    "bad_index = test[test.kitch_sq >= test.life_sq].index\n",
    "test.ix[bad_index, \"kitch_sq\"] = np.NaN\n",
    "bad_index = train[(train.kitch_sq == 0).values + (train.kitch_sq == 1).values].index\n",
    "train.ix[bad_index, \"kitch_sq\"] = np.NaN\n",
    "bad_index = test[(test.kitch_sq == 0).values + (test.kitch_sq == 1).values].index\n",
    "test.ix[bad_index, \"kitch_sq\"] = np.NaN\n",
    "bad_index = train[(train.full_sq > 210) & (train.life_sq / train.full_sq < 0.3)].index\n",
    "train.ix[bad_index, \"full_sq\"] = np.NaN\n",
    "bad_index = test[(test.full_sq > 150) & (test.life_sq / test.full_sq < 0.3)].index\n",
    "test.ix[bad_index, \"full_sq\"] = np.NaN\n",
    "bad_index = train[train.life_sq > 300].index\n",
    "train.ix[bad_index, [\"life_sq\", \"full_sq\"]] = np.NaN\n",
    "bad_index = test[test.life_sq > 200].index\n",
    "test.ix[bad_index, [\"life_sq\", \"full_sq\"]] = np.NaN\n",
    "bad_index = train[train.build_year < 1500].index\n",
    "train.ix[bad_index, \"build_year\"] = np.NaN\n",
    "bad_index = test[test.build_year < 1500].index\n",
    "test.ix[bad_index, \"build_year\"] = np.NaN\n",
    "bad_index = train[train.num_room == 0].index \n",
    "train.ix[bad_index, \"num_room\"] = np.NaN\n",
    "bad_index = test[test.num_room == 0].index \n",
    "test.ix[bad_index, \"num_room\"] = np.NaN\n",
    "bad_index = train[(train.floor == 0).values * (train.max_floor == 0).values].index\n",
    "train.ix[bad_index, [\"max_floor\", \"floor\"]] = np.NaN\n",
    "bad_index = train[train.floor == 0].index\n",
    "train.ix[bad_index, \"floor\"] = np.NaN\n",
    "bad_index = train[train.max_floor == 0].index\n",
    "train.ix[bad_index, \"max_floor\"] = np.NaN\n",
    "bad_index = test[test.max_floor == 0].index\n",
    "test.ix[bad_index, \"max_floor\"] = np.NaN\n",
    "bad_index = train[train.floor > train.max_floor].index\n",
    "train.ix[bad_index, \"max_floor\"] = np.NaN\n",
    "bad_index = test[test.floor > test.max_floor].index\n",
    "test.ix[bad_index, \"max_floor\"] = np.NaN\n",
    "bad_index = train[train.state == 33].index\n",
    "train.ix[bad_index, \"state\"] = np.NaN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extra Feature Addition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Add month-year\n",
    "train['timestamp'] = pd.to_datetime(train['timestamp'])\n",
    "month_year = (train.timestamp.dt.month + train.timestamp.dt.year * 100)\n",
    "month_year_cnt_map = month_year.value_counts().to_dict()\n",
    "train['month_year_cnt'] = month_year.map(month_year_cnt_map)\n",
    "test['timestamp'] = pd.to_datetime(test['timestamp'])\n",
    "month_year = (test.timestamp.dt.month + test.timestamp.dt.year * 100)\n",
    "month_year_cnt_map = month_year.value_counts().to_dict()\n",
    "test['month_year_cnt'] = month_year.map(month_year_cnt_map)\n",
    "# Add week-year count\n",
    "week_year = (train.timestamp.dt.weekofyear + train.timestamp.dt.year * 100)\n",
    "week_year_cnt_map = week_year.value_counts().to_dict()\n",
    "train['week_year_cnt'] = week_year.map(week_year_cnt_map)\n",
    "week_year = (test.timestamp.dt.weekofyear + test.timestamp.dt.year * 100)\n",
    "week_year_cnt_map = week_year.value_counts().to_dict()\n",
    "test['week_year_cnt'] = week_year.map(week_year_cnt_map)\n",
    "# Add month and day-of-week\n",
    "train['month'] = train.timestamp.dt.month\n",
    "train['dow'] = train.timestamp.dt.dayofweek\n",
    "test['month'] = test.timestamp.dt.month\n",
    "test['dow'] = test.timestamp.dt.dayofweek\n",
    "# Other feature engineering\n",
    "train['rel_floor'] = train['floor'] / train['max_floor'].astype(float)\n",
    "train['rel_kitch_sq'] = train['kitch_sq'] / train['full_sq'].astype(float)\n",
    "test['rel_floor'] = test['floor'] / test['max_floor'].astype(float)\n",
    "test['rel_kitch_sq'] = test['kitch_sq'] / test['full_sq'].astype(float)\n",
    "train['room_size'] = train['life_sq'] / train['num_room'].astype(float)\n",
    "test['room_size'] = test['life_sq'] / test['num_room'].astype(float)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Involve Macro Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "macro_cols = ['timestamp',\"balance_trade\", \"balance_trade_growth\", \"eurrub\", \"average_provision_of_build_contract\",\n",
    "\"micex_rgbi_tr\", \"micex_cbi_tr\", \"deposits_rate\", \"mortgage_value\", \"mortgage_rate\",\n",
    "\"income_per_cap\", \"rent_price_4+room_bus\", \"museum_visitis_per_100_cap\", \"apartment_build\"]\n",
    "macro = pd.read_csv('macro_c.csv')[macro_cols]\n",
    "train = train.merge(macro,how='left',on='timestamp')\n",
    "test = test.merge(macro,how='left',on='timestamp')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create PCA Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_fill = train.fillna(-999)\n",
    "test_fill = test.fillna(-999)\n",
    "from sklearn.decomposition import PCA\n",
    "n_comp = 20\n",
    "pca = PCA(n_components=n_comp,random_state=42)\n",
    "pca_results_train = pca.fit_transform(train_fill[col])\n",
    "pca_results_test = pca.transform(test_fill[col])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create ICA Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import FastICA\n",
    "ica = FastICA(n_components = n_comp,random_state = 42)\n",
    "ica_result_train = ica.fit_transform(train_fill[col])\n",
    "ica_result_test = ica.transform(test_fill[col])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Put features in original dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i in range(1, n_comp + 1):\n",
    "    train['pca_' + str(i)] = pca_results_train[:, i - 1]\n",
    "    test['pca_' + str(i)] = pca_results_test[:, i - 1]\n",
    "    train['ica_' + str(i)] = ica_result_train[:, i - 1]\n",
    "    test['ica_' + str(i)] = ica_result_test[:, i - 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ReducedVar = []\n",
    "for each in list(test.columns):\n",
    "    if 'pca' in each or 'ica' in each:\n",
    "        ReducedVar.append(each)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare a train/test set for Validating Stacking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_ = train.fillna(-999)\n",
    "test_ = test.fillna(-999)\n",
    "from sklearn.model_selection import train_test_split\n",
    "training,testing = train_test_split(train_,test_size = 0.2,random_state = 42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 1: ElasticNet Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import ElasticNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "col_new = list(test.columns)[2:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jiashen/env3/lib/python3.5/site-packages/sklearn/metrics/scorer.py:90: DeprecationWarning: Scoring method mean_squared_error was renamed to neg_mean_squared_error in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "/home/jiashen/env3/lib/python3.5/site-packages/sklearn/metrics/scorer.py:90: DeprecationWarning: Scoring method mean_squared_error was renamed to neg_mean_squared_error in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "/home/jiashen/env3/lib/python3.5/site-packages/sklearn/metrics/scorer.py:90: DeprecationWarning: Scoring method mean_squared_error was renamed to neg_mean_squared_error in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "/home/jiashen/env3/lib/python3.5/site-packages/sklearn/metrics/scorer.py:90: DeprecationWarning: Scoring method mean_squared_error was renamed to neg_mean_squared_error in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "/home/jiashen/env3/lib/python3.5/site-packages/sklearn/metrics/scorer.py:90: DeprecationWarning: Scoring method mean_squared_error was renamed to neg_mean_squared_error in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n"
     ]
    }
   ],
   "source": [
    "enet = ElasticNet(alpha=200,l1_ratio=1)\n",
    "cv = cross_val_score(enet,train_[ReducedVar],np.log(train_['price_doc']+1),scoring='mean_squared_error',cv=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.50443312,  0.45055124,  0.47046905,  0.4819424 ,  0.48129275])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.abs(cv)**0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.478058888877\n"
     ]
    }
   ],
   "source": [
    "print(np.mean(np.abs(cv))**0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Fit the model and check the squared error**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "enet.fit(training[ReducedVar],np.log(training['price_doc']+1))\n",
    "pred_enet = enet.predict(testing[ReducedVar])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ResultSingleModel = pd.DataFrame({'id':range(len(pred_enet)),'ElasticNet':pred_enet})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model2: Random Forest Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rf = RandomForestRegressor(n_estimators=150,random_state=42)\n",
    "#cv = cross_val_score(rf,train_[ReducedVar],np.log(train_['price_doc']+1),scoring='mean_squared_error',cv=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.42918294450179062"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(np.abs(cv)**0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Fit the model and check the error**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rf.fit(training[ReducedVar],np.log(training['price_doc']+1))\n",
    "pred_rf = rf.predict(testing[ReducedVar])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ResultSingleModel['RandomForest'] = pred_rf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 3: Gradient Boosting Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "gbt = GradientBoostingRegressor(learning_rate = 0.05,n_estimators = 300,max_depth = 6,max_features = 'auto')\n",
    "#cv = cross_val_score(gbt,train_[ReducedVar],np.log(train_['price_doc']+1),scoring='mean_squared_error',cv=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.43029469446315727"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(np.abs(cv)**0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Fit the model and check the error**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gbt.fit(training[ReducedVar],np.log(training['price_doc']+1))\n",
    "pred_gbt = gbt.predict(testing[ReducedVar])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ResultSingleModel['GradientBoosting'] = pred_gbt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ResultSingleModel['Actual'] = list(np.log(testing['price_doc']+1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 4: xgboost, For Comparison only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dtrain = xgb.DMatrix(train[ReducedVar],np.log(train['price_doc']+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "params = {\n",
    "    'eta': 0.05, ## Try 0.01,3,5\n",
    "    'max_depth': 3,## Try 4,5,6\n",
    "    'subsample': 0.7,\n",
    "    'colsample_bytree': 0.7,\n",
    "    'objective': 'reg:linear',\n",
    "    'eval_metric': 'rmse',\n",
    "    'silent': 1,\n",
    "    'lambda':0.5,\n",
    "    'min_child_weight':3\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttrain-rmse:14.427\ttest-rmse:14.427\n",
      "[50]\ttrain-rmse:1.19881\ttest-rmse:1.20022\n",
      "[100]\ttrain-rmse:0.447184\ttest-rmse:0.453489\n",
      "[150]\ttrain-rmse:0.430291\ttest-rmse:0.439573\n",
      "[200]\ttrain-rmse:0.423986\ttest-rmse:0.436189\n",
      "[250]\ttrain-rmse:0.418447\ttest-rmse:0.433741\n",
      "[300]\ttrain-rmse:0.413627\ttest-rmse:0.431501\n",
      "[350]\ttrain-rmse:0.409287\ttest-rmse:0.430029\n",
      "[400]\ttrain-rmse:0.405461\ttest-rmse:0.428672\n",
      "[450]\ttrain-rmse:0.401767\ttest-rmse:0.427654\n",
      "[500]\ttrain-rmse:0.398329\ttest-rmse:0.426668\n",
      "[550]\ttrain-rmse:0.395125\ttest-rmse:0.425952\n",
      "[600]\ttrain-rmse:0.391921\ttest-rmse:0.425195\n",
      "[650]\ttrain-rmse:0.389302\ttest-rmse:0.424639\n",
      "[700]\ttrain-rmse:0.386791\ttest-rmse:0.424383\n",
      "[750]\ttrain-rmse:0.384164\ttest-rmse:0.424009\n",
      "[800]\ttrain-rmse:0.381629\ttest-rmse:0.42362\n",
      "[850]\ttrain-rmse:0.379199\ttest-rmse:0.423274\n",
      "[900]\ttrain-rmse:0.376868\ttest-rmse:0.422954\n"
     ]
    }
   ],
   "source": [
    "xgbcv = xgb.cv(params, dtrain, num_boost_round=1000, early_stopping_rounds=20,\n",
    "    verbose_eval=50, show_stdv=False,seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "919\n"
     ]
    }
   ],
   "source": [
    "print(len(xgbcv))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dtraining = xgb.DMatrix(training[ReducedVar],np.log(training['price_doc']+1))\n",
    "dtesting = xgb.DMatrix(testing[ReducedVar])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "xgb_model = xgb.train(params,dtraining,num_boost_round=len(xgbcv))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "xgb_predict = xgb_model.predict(dtesting)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ResultSingleModel['xgb'] = xgb_predict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## See whether this one will be good for stacking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ResultSingleModel = pd.read_csv('BeforeStacking.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ResultSingleModel['ElasticNet'] = np.abs(ResultSingleModel['Actual'] - ResultSingleModel['ElasticNet'])\n",
    "ResultSingleModel['RandomForest'] = np.abs(ResultSingleModel['Actual'] - ResultSingleModel['RandomForest'])\n",
    "ResultSingleModel['GradientBoosting'] = np.abs(ResultSingleModel['Actual'] - ResultSingleModel['GradientBoosting'])\n",
    "ResultSingleModel['xgb'] = np.abs(ResultSingleModel['Actual'] - ResultSingleModel['xgb'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ResultSingleModel = ResultSingleModel[['ElasticNet','RandomForest','GradientBoosting','xgb']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Best_index = list(np.argmin(ResultSingleModel.as_matrix(),axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({0: 1390, 1: 827, 2: 931, 3: 2705})"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "Counter(Best_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**We can, actually, do stacking on three models,as they perform quite differently on the testing set.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stacking Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "enet_feature = enet.fit(train_[ReducedVar],np.log(train_['price_doc']+1)).predict(train_[ReducedVar])\n",
    "rf_feature = rf.fit(train_[ReducedVar],np.log(train_['price_doc']+1)).predict(train_[ReducedVar])\n",
    "gbdt_feature = gbt.fit(train_[ReducedVar],np.log(train_['price_doc']+1)).predict(train_[ReducedVar])\n",
    "xgb_feature = xgb.train(params,dtrain,num_boost_round=919).predict(dtrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stack_frame = pd.DataFrame({'ENet':enet_feature,'RF':rf_feature,'GBDT':gbdt_feature,'xgb':xgb_feature})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Average stacking data frame**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stack_frame['average_value'] = (stack_frame['ENet']+stack_frame['RF']+stack_frame['GBDT']+stack_frame['xgb'])/4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Do a small validation on the model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "enet_feature = enet.fit(training[ReducedVar],np.log(training['price_doc']+1)).predict(training[ReducedVar])\n",
    "rf_feature = rf.fit(training[ReducedVar],np.log(training['price_doc']+1)).predict(training[ReducedVar])\n",
    "gbdt_feature = gbt.fit(training[ReducedVar],np.log(training['price_doc']+1)).predict(training[ReducedVar])\n",
    "xgb_feature =  xgb.train(params,dtraining,num_boost_round=919).predict(dtraining)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "enet_predictor = enet.fit(training[ReducedVar],np.log(training['price_doc']+1)).predict(testing[ReducedVar])\n",
    "rf_predictor = rf.fit(training[ReducedVar],np.log(training['price_doc']+1)).predict(testing[ReducedVar])\n",
    "gbdt_predictor = gbt.fit(training[ReducedVar],np.log(training['price_doc']+1)).predict(testing[ReducedVar])\n",
    "xgb_prdictor =  xgb.train(params,dtraining,num_boost_round=919).predict(dtesting)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stack_train = pd.DataFrame({'ENet':enet_feature,'RF':rf_feature,'GBDT':gbdt_feature,'xgb':xgb_feature})\n",
    "stack_test = pd.DataFrame({'ENet':enet_predictor,'RF':rf_predictor,'GBDT':gbdt_predictor,'xgb':xgb_prdictor})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stack_test['Averaged'] = (stack_test['ENet'] + stack_test['RF'] + stack_test['GBDT'] + stack_test['xgb'])/4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "lr = LinearRegression()\n",
    "pred = lr.fit(stack_train,np.log(training['price_doc']+1)).predict(stack_test.drop('Averaged',axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ENet 0.476690677399\n",
      "GBDT 0.417996143386\n",
      "RF 0.413094700027\n",
      "xgb 0.420923889073\n",
      "Averaged 0.411393674364\n"
     ]
    }
   ],
   "source": [
    "for each in list(stack_test.columns):\n",
    "    print(each+' '+str(mean_squared_error(np.log(testing['price_doc']+1),stack_test[each])**0.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
