{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%run Functions.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv('data/train_c.csv')\n",
    "test = pd.read_csv('data/test_c.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train,test = get_additional_features(train,test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "col = test.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "rf1 = RandomForestRegressor(n_estimators=1000,max_features=0.95,max_depth=2,n_jobs=-1,random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training 1 Fold\n",
      "R2 Scored of Fold 1 is 0.594718792522\n",
      "RMSE of Fold 1 is 7.94241748355\n",
      "Training 2 Fold\n",
      "R2 Scored of Fold 2 is 0.436209390353\n",
      "RMSE of Fold 2 is 10.301804413\n",
      "Training 3 Fold\n",
      "R2 Scored of Fold 3 is 0.597217297038\n",
      "RMSE of Fold 3 is 7.8938036502\n",
      "Training 4 Fold\n",
      "R2 Scored of Fold 4 is 0.610218106797\n",
      "RMSE of Fold 4 is 7.60267070007\n",
      "Training 5 Fold\n",
      "R2 Scored of Fold 5 is 0.597466264537\n",
      "RMSE of Fold 5 is 7.90263448083\n",
      "Start Training\n",
      "Calculating In-Bag R2 Score\n",
      "0.564618508279\n",
      "Calculating Out-Bag R2 Score\n",
      "0.567165970249\n",
      "Calculating In-Bag RMSE\n",
      "8.36529856463\n",
      "Calculating Out-Bag RMSE\n",
      "8.32866614553\n"
     ]
    }
   ],
   "source": [
    "train = train.sample(frac = 1,random_state=42)\n",
    "rf1_train,rf1_test,ID = get_sklearn_stack_data(rf1,train,col,train['y'],test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "save_results(rf1_train,rf1_test,'RandomForest_depth2_0.5646_0.5672.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gb1 = GradientBoostingRegressor(n_estimators=1000,max_features=0.95,learning_rate=0.005,max_depth=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training 1 Fold\n",
      "R2 Scored of Fold 1 is 0.568203718172\n",
      "RMSE of Fold 1 is 8.19811351502\n",
      "Training 2 Fold\n",
      "R2 Scored of Fold 2 is 0.437719255244\n",
      "RMSE of Fold 2 is 10.2880007416\n",
      "Training 3 Fold\n",
      "R2 Scored of Fold 3 is 0.603689388526\n",
      "RMSE of Fold 3 is 7.83012624234\n",
      "Training 4 Fold\n",
      "R2 Scored of Fold 4 is 0.569305379461\n",
      "RMSE of Fold 4 is 7.9917165755\n",
      "Training 5 Fold\n",
      "R2 Scored of Fold 5 is 0.573245081584\n",
      "RMSE of Fold 5 is 8.13691952644\n",
      "Start Training\n",
      "Calculating In-Bag R2 Score\n",
      "0.691918117216\n",
      "Calculating Out-Bag R2 Score\n",
      "0.550432564597\n",
      "Calculating In-Bag RMSE\n",
      "7.03687026532\n",
      "Calculating Out-Bag RMSE\n",
      "8.48897532018\n"
     ]
    }
   ],
   "source": [
    "gb1_train,gb1_test,ID = get_sklearn_stack_data(gb1,train,col,train['y'],test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "save_results(gb1_train,gb1_test,'GBDT_1000trees_depth4_0.55_0.69.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lasso Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Lasso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "las1 = Lasso(alpha=5,random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training 1 Fold\n",
      "R2 Scored of Fold 1 is 0.603607303776\n",
      "RMSE of Fold 1 is 7.8548392245\n",
      "Training 2 Fold\n",
      "R2 Scored of Fold 2 is 0.464060815042\n",
      "RMSE of Fold 2 is 10.0441256478\n",
      "Training 3 Fold\n",
      "R2 Scored of Fold 3 is 0.61069213745\n",
      "RMSE of Fold 3 is 7.76063934172\n",
      "Training 4 Fold\n",
      "R2 Scored of Fold 4 is 0.612605885741\n",
      "RMSE of Fold 4 is 7.57934818996\n",
      "Training 5 Fold\n",
      "R2 Scored of Fold 5 is 0.603487405026\n",
      "RMSE of Fold 5 is 7.843307586\n",
      "Start Training\n",
      "Calculating In-Bag R2 Score\n",
      "0.575435112946\n",
      "Calculating Out-Bag R2 Score\n",
      "0.578890709407\n",
      "Calculating In-Bag RMSE\n",
      "8.26073140125\n",
      "Calculating Out-Bag RMSE\n",
      "8.21645199799\n"
     ]
    }
   ],
   "source": [
    "las1_train,las1_test,ID = get_sklearn_stack_data(las1,train,col,train['y'],test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "save_results(las1_train,las1_test,'Lasso_Lambda5_0.5754_0.5788.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## xgboost gb:linear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_mean = np.mean(train.y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "params = {\n",
    "    'booster': 'gblinear', \n",
    "    'eta': 0.005,\n",
    "    'objective': 'reg:linear',\n",
    "    'eval_metric': 'rmse',\n",
    "    'base_score': y_mean, # base prediction = mean(target)\n",
    "    'silent': 1,\n",
    "    'alpha':20\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dtrain = xgb.DMatrix(train.drop('y', axis=1), train.y)\n",
    "dtest = xgb.DMatrix(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttrain-rmse:12.3731+0.307918\ttest-rmse:12.3693+0.606195\n",
      "[50]\ttrain-rmse:9.24611+0.401868\ttest-rmse:9.40644+0.755497\n",
      "[100]\ttrain-rmse:8.77504+0.416331\ttest-rmse:9.01093+0.771294\n",
      "[150]\ttrain-rmse:8.55053+0.422751\ttest-rmse:8.83131+0.773093\n",
      "[200]\ttrain-rmse:8.42613+0.426376\ttest-rmse:8.73782+0.770957\n",
      "[250]\ttrain-rmse:8.34977+0.428857\ttest-rmse:8.68414+0.768123\n",
      "[300]\ttrain-rmse:8.2991+0.430706\ttest-rmse:8.65085+0.765619\n",
      "[350]\ttrain-rmse:8.26345+0.432159\ttest-rmse:8.62886+0.76363\n",
      "[400]\ttrain-rmse:8.23723+0.433267\ttest-rmse:8.61348+0.761971\n",
      "[450]\ttrain-rmse:8.21729+0.434089\ttest-rmse:8.6022+0.760572\n",
      "[500]\ttrain-rmse:8.20172+0.434716\ttest-rmse:8.59362+0.759335\n",
      "[550]\ttrain-rmse:8.18929+0.43522\ttest-rmse:8.58694+0.758252\n",
      "[600]\ttrain-rmse:8.17918+0.435651\ttest-rmse:8.58171+0.757361\n",
      "[650]\ttrain-rmse:8.17082+0.436026\ttest-rmse:8.57754+0.756621\n",
      "[700]\ttrain-rmse:8.16382+0.436357\ttest-rmse:8.57418+0.756006\n",
      "[750]\ttrain-rmse:8.15788+0.43665\ttest-rmse:8.57143+0.755502\n",
      "[800]\ttrain-rmse:8.15278+0.436912\ttest-rmse:8.56916+0.755097\n",
      "[850]\ttrain-rmse:8.14838+0.437138\ttest-rmse:8.56727+0.754778\n",
      "[900]\ttrain-rmse:8.14454+0.437331\ttest-rmse:8.56568+0.754535\n",
      "[950]\ttrain-rmse:8.14117+0.437495\ttest-rmse:8.56432+0.754349\n",
      "[1000]\ttrain-rmse:8.13819+0.437639\ttest-rmse:8.56317+0.754205\n",
      "[1050]\ttrain-rmse:8.13552+0.437764\ttest-rmse:8.5622+0.754078\n",
      "[1100]\ttrain-rmse:8.13312+0.437875\ttest-rmse:8.56139+0.753945\n",
      "[1150]\ttrain-rmse:8.13095+0.437971\ttest-rmse:8.56071+0.753826\n",
      "[1200]\ttrain-rmse:8.12899+0.438057\ttest-rmse:8.56011+0.753743\n",
      "[1250]\ttrain-rmse:8.1272+0.438141\ttest-rmse:8.55958+0.753694\n",
      "[1300]\ttrain-rmse:8.12555+0.438223\ttest-rmse:8.55913+0.75366\n",
      "[1350]\ttrain-rmse:8.12405+0.438304\ttest-rmse:8.55878+0.753623\n",
      "[1400]\ttrain-rmse:8.12265+0.438381\ttest-rmse:8.55852+0.753588\n",
      "[1450]\ttrain-rmse:8.12136+0.438455\ttest-rmse:8.55834+0.753577\n",
      "[1500]\ttrain-rmse:8.12016+0.438525\ttest-rmse:8.55819+0.753593\n",
      "[1550]\ttrain-rmse:8.11904+0.438593\ttest-rmse:8.55808+0.753633\n",
      "[1600]\ttrain-rmse:8.11799+0.438661\ttest-rmse:8.55797+0.753706\n",
      "[1650]\ttrain-rmse:8.117+0.438729\ttest-rmse:8.55787+0.753797\n",
      "[1700]\ttrain-rmse:8.11608+0.438795\ttest-rmse:8.55777+0.753895\n",
      "[1750]\ttrain-rmse:8.1152+0.438862\ttest-rmse:8.55768+0.753984\n",
      "[1800]\ttrain-rmse:8.11437+0.438926\ttest-rmse:8.55758+0.754055\n",
      "[1850]\ttrain-rmse:8.11359+0.43899\ttest-rmse:8.55749+0.754115\n",
      "[1900]\ttrain-rmse:8.11284+0.439053\ttest-rmse:8.5574+0.754168\n",
      "[1950]\ttrain-rmse:8.11212+0.439116\ttest-rmse:8.55732+0.754214\n",
      "[2000]\ttrain-rmse:8.11144+0.439181\ttest-rmse:8.55724+0.754254\n",
      "[2050]\ttrain-rmse:8.11078+0.439246\ttest-rmse:8.55718+0.754296\n",
      "[2100]\ttrain-rmse:8.11015+0.439311\ttest-rmse:8.55712+0.75434\n",
      "[2150]\ttrain-rmse:8.10955+0.439375\ttest-rmse:8.55708+0.754385\n",
      "[2200]\ttrain-rmse:8.10896+0.439436\ttest-rmse:8.55706+0.754431\n",
      "[2250]\ttrain-rmse:8.10841+0.439493\ttest-rmse:8.55704+0.754477\n",
      "[2300]\ttrain-rmse:8.10787+0.439546\ttest-rmse:8.55704+0.754521\n",
      "Performance does not improve from 2289 rounds\n"
     ]
    }
   ],
   "source": [
    "xgb_cvalid = xgb.cv(params, dtrain, num_boost_round=3000, early_stopping_rounds=20,\n",
    "    verbose_eval=50, show_stdv=True,seed=42)\n",
    "xgb_cvalid[['train-rmse-mean', 'test-rmse-mean']].plot()\n",
    "print('Performance does not improve from '+str(len(xgb_cvalid))+' rounds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training 1 Fold\n",
      "R2 Scored of Fold 1 is 0.582538670736\n",
      "RMSE of Fold 1 is 8.06088274967\n",
      "Training 2 Fold\n",
      "R2 Scored of Fold 2 is 0.42429201895\n",
      "RMSE of Fold 2 is 10.4101144987\n",
      "Training 3 Fold\n",
      "R2 Scored of Fold 3 is 0.575668856365\n",
      "RMSE of Fold 3 is 8.10220761373\n",
      "Training 4 Fold\n",
      "R2 Scored of Fold 4 is 0.592077123265\n",
      "RMSE of Fold 4 is 7.77757809346\n",
      "Training 5 Fold\n",
      "R2 Scored of Fold 5 is 0.57902903784\n",
      "RMSE of Fold 5 is 8.08159017731\n",
      "Start Training\n",
      "Calculating In-Bag R2 Score\n",
      "0.578045021378\n",
      "Calculating Out-Bag R2 Score\n",
      "0.550721141431\n",
      "Calculating In-Bag RMSE\n",
      "8.23530147777\n",
      "Calculating Out-Bag RMSE\n",
      "8.48647462657\n"
     ]
    }
   ],
   "source": [
    "xgblinear_stack,xgb_linearpred,ID = get_xgb_stack_data(params,2289,train,col,train.y,test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "save_results(xgblinear_stack,xgb_linearpred,'xgb_gblinear_Rounds2289_0.5507_0.5780.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stacking LGBM, Lasso and GBDM then Average with XGB Best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lgbm_train,lgbm_test = read_data('sublgb_depth4_0.56_0.63_All_decomp_Magic_X0_ID.csv')\n",
    "gbt_train,gbt_test = read_data('subGBDT_1000trees_depth4_0.55_0.69.csv')\n",
    "las_train,las_test = read_data('subLasso_Lambda5_0.5754_0.5788.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load xgb train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "xgb_train,xgb_test = read_data('subxgb_depth4_581Rounds_All_Decomp_magic_features_0.5526_0.6378.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Put four predicted values in a single dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lgbm_train = pd.DataFrame({'ID':lgbm_train['ID'],'model1':lgbm_train['predicted']})\n",
    "gbt_train = pd.DataFrame({'ID':gbt_train['ID'],'model2':gbt_train['predicted']})\n",
    "las_train = pd.DataFrame({'ID':las_train['ID'],'model3':las_train['predicted'],'label':las1_train['label']})\n",
    "stack_train = lgbm_train.merge(gbt_train,on='ID',how='left')\n",
    "stack_train = stack_train.merge(las_train,on='ID',how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stack_test = pd.DataFrame({'ID':las_test['ID'],'model1':lgbm_test['y'],'model2':gbt_test['y'],'model3':las_test['y']})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.1555860694175064"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MSE(stack_train['model1'],stack_train['model2'])**0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.7523737578350715"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MSE(stack_train['model1'],stack_train['model3'])**0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.3185563971742806"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MSE(stack_train['model2'],stack_train['model3'])**0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Do Stacking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "col = list(stack_test.columns)[1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**xgb as Level2**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "params = {\n",
    "    'eta': 0.005,\n",
    "    'max_depth': 2,\n",
    "    'objective': 'reg:linear',\n",
    "    'eval_metric': 'rmse',\n",
    "    'base_score': y_mean, # base prediction = mean(target)\n",
    "    'silent': 1\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dtrain = xgb.DMatrix(stack_train[col], stack_train['label'])\n",
    "dtest = xgb.DMatrix(stack_test[col])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttrain-rmse:12.6393+0.276661\ttest-rmse:12.6306+0.546747\n",
      "[50]\ttrain-rmse:11.1513+0.315604\ttest-rmse:11.1481+0.606903\n",
      "[100]\ttrain-rmse:10.1362+0.348351\ttest-rmse:10.1405+0.664943\n",
      "[150]\ttrain-rmse:9.46144+0.375123\ttest-rmse:9.47237+0.714573\n",
      "[200]\ttrain-rmse:9.01967+0.39547\ttest-rmse:9.03701+0.755494\n",
      "[250]\ttrain-rmse:8.73225+0.409114\ttest-rmse:8.75641+0.786875\n",
      "[300]\ttrain-rmse:8.54457+0.418856\ttest-rmse:8.58127+0.812517\n",
      "[350]\ttrain-rmse:8.41962+0.425887\ttest-rmse:8.46968+0.830191\n",
      "[400]\ttrain-rmse:8.33491+0.4308\ttest-rmse:8.40043+0.843365\n",
      "[450]\ttrain-rmse:8.2771+0.434383\ttest-rmse:8.35562+0.85248\n",
      "[500]\ttrain-rmse:8.23688+0.436748\ttest-rmse:8.32768+0.859046\n",
      "[550]\ttrain-rmse:8.20816+0.438249\ttest-rmse:8.31048+0.86311\n",
      "[600]\ttrain-rmse:8.186+0.439478\ttest-rmse:8.29975+0.865653\n",
      "[650]\ttrain-rmse:8.16853+0.44021\ttest-rmse:8.29276+0.866383\n",
      "[700]\ttrain-rmse:8.15156+0.439319\ttest-rmse:8.28927+0.865703\n",
      "[750]\ttrain-rmse:8.13666+0.437439\ttest-rmse:8.2873+0.865041\n",
      "Performance does not improve from 777 rounds\n"
     ]
    }
   ],
   "source": [
    "xgb_cvalid = xgb.cv(params, dtrain, num_boost_round=2000, early_stopping_rounds=20,\n",
    "    verbose_eval=50, show_stdv=True,seed=42)\n",
    "xgb_cvalid[['train-rmse-mean', 'test-rmse-mean']].plot()\n",
    "print('Performance does not improve from '+str(len(xgb_cvalid))+' rounds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training 1 Fold\n",
      "R2 Scored of Fold 1 is 0.578095592816\n",
      "RMSE of Fold 1 is 8.57702372596\n",
      "Training 2 Fold\n",
      "R2 Scored of Fold 2 is 0.484598268616\n",
      "RMSE of Fold 2 is 9.62303230801\n",
      "Training 3 Fold\n",
      "R2 Scored of Fold 3 is 0.582557754053\n",
      "RMSE of Fold 3 is 7.99946988521\n",
      "Training 4 Fold\n",
      "R2 Scored of Fold 4 is 0.654525182047\n",
      "RMSE of Fold 4 is 7.11136474292\n",
      "Training 5 Fold\n",
      "R2 Scored of Fold 5 is 0.565147909149\n",
      "RMSE of Fold 5 is 8.05278075299\n",
      "Start Training\n",
      "Calculating In-Bag R2 Score\n",
      "0.590568687234\n",
      "Calculating Out-Bag R2 Score\n",
      "0.572984941336\n",
      "Calculating In-Bag RMSE\n",
      "8.11216919964\n",
      "Calculating Out-Bag RMSE\n",
      "8.27273428302\n"
     ]
    }
   ],
   "source": [
    "Stack_Three_train,Stack_Three_pred,ID = get_xgb_stack_data(params,777,stack_train,col,stack_train['label'],stack_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "save_results(Stack_Three_train,Stack_Three_pred,'Stacking_gbdt_lgbm_lasso_with_xgb_rounds_777.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Average Stacking With xgb_best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "xgb_train = xgb_train[['ID','predicted']]\n",
    "xgb_train.columns = ['ID','stacking']\n",
    "xgb_train.columns = ['ID','model4']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Average_train = Stack_Three_train[['ID','predicted','label']].merge(xgb_train,on='ID',how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Average_test = Stack_Three_pred\n",
    "Average_test.columns = ['ID','stacking']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Average_test['predicted'] = xgb_test['y']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def weight_data(a,log=False):\n",
    "    if log:\n",
    "        Res = np.exp(a*np.log(Average_train['stacking'])+(1-a)*np.log(Average_train['predicted']))\n",
    "    else:\n",
    "        Res = a*Average_train['stacking'] + (1-a)*Average_train['predicted']\n",
    "    print(MSE(Average_train['label'],Res)**0.5)\n",
    "    print(r2_score(Average_train['label'],Res)**0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8.36748668293\n",
      "0.751259417426\n"
     ]
    }
   ],
   "source": [
    "weight_data(0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8.32572114227\n",
      "0.754140879991\n",
      "8.32762492163\n",
      "0.754010088234\n"
     ]
    }
   ],
   "source": [
    "weight_data(0.2)\n",
    "weight_data(0.22)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a = 0.25\n",
    "Final = a*Average_test['stacking'] + (1-a)*Average_test['predicted']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sub = pd.DataFrame({'ID':test['ID'],'y':Final})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sub.to_csv('submission/weighted_Stacking_bestxgb_Stacking_0.25.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stack also xgb and rf, then using three different level 2 algorithm before averaging them on level 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stack_train = stack_train.merge(xgb_train,on='ID',how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rf_train, rf_test = read_data('subRandomForest_depth2_0.5646_0.5672.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rf_train = rf_train[['ID','predicted']]\n",
    "rf_train.columns = ['ID','model5']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stack_train = stack_train.merge(rf_train,on='ID',how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def Mean_error(list1,list2):\n",
    "    diff = [x - y for x,y in zip(list1,list2)]\n",
    "    return np.mean(diff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Error\n",
      "-0.0206285253464\n",
      "RMSE\n",
      "8.41581005274\n",
      "R2_Score\n",
      "0.559344778381\n",
      "Mean Error\n",
      "-0.0961229167594\n",
      "RMSE\n",
      "8.53753861785\n",
      "R2_Score\n",
      "0.546505072722\n",
      "Mean Error\n",
      "-0.00228340533135\n",
      "RMSE\n",
      "8.26780016108\n",
      "R2_Score\n",
      "0.574708196375\n",
      "Mean Error\n",
      "-0.0401703491356\n",
      "RMSE\n",
      "8.49895704984\n",
      "R2_Score\n",
      "0.550594544471\n",
      "Mean Error\n",
      "-0.00691504290748\n",
      "RMSE\n",
      "8.38788325541\n",
      "R2_Score\n",
      "0.562264442892\n"
     ]
    }
   ],
   "source": [
    "for i in range(1,6):\n",
    "    model = 'model'+str(i)\n",
    "    label = stack_train['label']\n",
    "    pred = stack_train[model]\n",
    "    print('Mean Error')\n",
    "    print(Mean_error(label,pred))\n",
    "    print('RMSE')\n",
    "    print(MSE(label,pred)**0.5)\n",
    "    print('R2_Score')\n",
    "    print(r2_score(label,pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stack_test['model4'] = xgb_test['y']\n",
    "stack_test['model5'] = rf_test['y']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "col = list(stack_test.columns)[1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Meta 1: xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "params = {\n",
    "    'eta': 0.005,\n",
    "    'max_depth': 2,\n",
    "    'objective': 'reg:linear',\n",
    "    'eval_metric': 'rmse',\n",
    "    'base_score': y_mean, # base prediction = mean(target)\n",
    "    'silent': 1\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dtrain = xgb.DMatrix(stack_train[col], stack_train['label'])\n",
    "dtest = xgb.DMatrix(stack_test[col])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttrain-rmse:12.6393+0.276661\ttest-rmse:12.6306+0.546747\n",
      "[50]\ttrain-rmse:11.1513+0.315601\ttest-rmse:11.1481+0.60695\n",
      "[100]\ttrain-rmse:10.1361+0.348313\ttest-rmse:10.1405+0.665117\n",
      "[150]\ttrain-rmse:9.46125+0.374988\ttest-rmse:9.47269+0.714465\n",
      "[200]\ttrain-rmse:9.0192+0.395139\ttest-rmse:9.03748+0.7551\n",
      "[250]\ttrain-rmse:8.73144+0.408623\ttest-rmse:8.75664+0.786292\n",
      "[300]\ttrain-rmse:8.54331+0.418168\ttest-rmse:8.58081+0.811773\n",
      "[350]\ttrain-rmse:8.41782+0.425127\ttest-rmse:8.4682+0.829371\n",
      "[400]\ttrain-rmse:8.33267+0.429943\ttest-rmse:8.39856+0.841971\n",
      "[450]\ttrain-rmse:8.2745+0.433365\ttest-rmse:8.3535+0.850414\n",
      "[500]\ttrain-rmse:8.23386+0.43558\ttest-rmse:8.32614+0.856924\n",
      "[550]\ttrain-rmse:8.2039+0.436383\ttest-rmse:8.3096+0.860843\n",
      "[600]\ttrain-rmse:8.18117+0.437051\ttest-rmse:8.29864+0.8637\n",
      "[650]\ttrain-rmse:8.16292+0.437412\ttest-rmse:8.29216+0.864659\n",
      "[700]\ttrain-rmse:8.1412+0.434977\ttest-rmse:8.28862+0.864294\n",
      "[750]\ttrain-rmse:8.12225+0.431416\ttest-rmse:8.28717+0.863467\n",
      "[800]\ttrain-rmse:8.10755+0.427249\ttest-rmse:8.28667+0.862805\n",
      "Performance does not improve from 803 rounds\n"
     ]
    }
   ],
   "source": [
    "xgb_cvalid = xgb.cv(params, dtrain, num_boost_round=2000, early_stopping_rounds=20,\n",
    "    verbose_eval=50, show_stdv=True,seed=42)\n",
    "xgb_cvalid[['train-rmse-mean', 'test-rmse-mean']].plot()\n",
    "print('Performance does not improve from '+str(len(xgb_cvalid))+' rounds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training 1 Fold\n",
      "R2 Scored of Fold 1 is 0.578655565822\n",
      "RMSE of Fold 1 is 8.5713299048\n",
      "Training 2 Fold\n",
      "R2 Scored of Fold 2 is 0.484772004839\n",
      "RMSE of Fold 2 is 9.62141026243\n",
      "Training 3 Fold\n",
      "R2 Scored of Fold 3 is 0.582550223458\n",
      "RMSE of Fold 3 is 7.9995420395\n",
      "Training 4 Fold\n",
      "R2 Scored of Fold 4 is 0.654879519434\n",
      "RMSE of Fold 4 is 7.10771690996\n",
      "Training 5 Fold\n",
      "R2 Scored of Fold 5 is 0.564033077039\n",
      "RMSE of Fold 5 is 8.06309661923\n",
      "Start Training\n",
      "Calculating In-Bag R2 Score\n",
      "0.58976897611\n",
      "Calculating Out-Bag R2 Score\n",
      "0.572978078119\n",
      "Calculating In-Bag RMSE\n",
      "8.12008753236\n",
      "Calculating Out-Bag RMSE\n",
      "8.27261914718\n"
     ]
    }
   ],
   "source": [
    "xgb_level2_train,xgb_level2_test,ID = get_xgb_stack_data(params,803,stack_train,col,stack_train['label'],stack_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Meta 2: Lasso Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training 1 Fold\n",
      "R2 Scored of Fold 1 is 0.581512295495\n",
      "RMSE of Fold 1 is 8.54222353039\n",
      "Training 2 Fold\n",
      "R2 Scored of Fold 2 is 0.483842124283\n",
      "RMSE of Fold 2 is 9.63008868177\n",
      "Training 3 Fold\n",
      "R2 Scored of Fold 3 is 0.578875283937\n",
      "RMSE of Fold 3 is 8.03467610653\n",
      "Training 4 Fold\n",
      "R2 Scored of Fold 4 is 0.662492324792\n",
      "RMSE of Fold 4 is 7.02888734361\n",
      "Training 5 Fold\n",
      "R2 Scored of Fold 5 is 0.586485928938\n",
      "RMSE of Fold 5 is 7.85272230675\n",
      "Start Training\n",
      "Calculating In-Bag R2 Score\n",
      "0.576500406829\n",
      "Calculating Out-Bag R2 Score\n",
      "0.578641591489\n",
      "Calculating In-Bag RMSE\n",
      "8.25036121518\n",
      "Calculating Out-Bag RMSE\n",
      "8.21771959381\n"
     ]
    }
   ],
   "source": [
    "lasso_level2_train,lasso_level2_test,ID = get_sklearn_stack_data(Lasso(alpha=0.1,random_state=42),stack_train,col,stack_train['label'],stack_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Meta 3: Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training 1 Fold\n",
      "R2 Scored of Fold 1 is 0.579512163055\n",
      "RMSE of Fold 1 is 8.56261267447\n",
      "Training 2 Fold\n",
      "R2 Scored of Fold 2 is 0.484280098229\n",
      "RMSE of Fold 2 is 9.62600211908\n",
      "Training 3 Fold\n",
      "R2 Scored of Fold 3 is 0.580777719108\n",
      "RMSE of Fold 3 is 8.01650719839\n",
      "Training 4 Fold\n",
      "R2 Scored of Fold 4 is 0.65307364588\n",
      "RMSE of Fold 4 is 7.12628852712\n",
      "Training 5 Fold\n",
      "R2 Scored of Fold 5 is 0.560521796059\n",
      "RMSE of Fold 5 is 8.09550162391\n",
      "Start Training\n",
      "Calculating In-Bag R2 Score\n",
      "0.600413660334\n",
      "Calculating Out-Bag R2 Score\n",
      "0.571633084466\n",
      "Calculating In-Bag RMSE\n",
      "8.01404507639\n",
      "Calculating Out-Bag RMSE\n",
      "8.28538242859\n"
     ]
    }
   ],
   "source": [
    "gbdt_level2_train,gbdt_level2_test,ID = get_sklearn_stack_data(GradientBoostingRegressor(n_estimators=100,subsample=0.95,max_features=1,max_depth=2,random_state=42),stack_train,col,stack_train['label'],stack_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Meta 4: LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%run Functions.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "params = params = {\n",
    "        'objective': 'regression',\n",
    "            'metric': 'rmse',\n",
    "            'boosting': 'gbdt',\n",
    "            'learning_rate': 0.0045 , #small learn rate, large number of iterations\n",
    "            'verbose': 0,\n",
    "            'num_iterations': 500,\n",
    "            'bagging_fraction': 0.95,\n",
    "            'bagging_freq': 1,\n",
    "            'bagging_seed': 42,\n",
    "            'feature_fraction': 0.95,\n",
    "            'feature_fraction_seed': 42,\n",
    "            'max_bin': 100,\n",
    "            'max_depth': 4,\n",
    "            'num_rounds': 1200\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training 1 Fold\n",
      "R2 Scored of Fold 1 is 0.578675688729\n",
      "RMSE of Fold 1 is 8.57112522412\n",
      "Training 2 Fold\n",
      "R2 Scored of Fold 2 is 0.481436756758\n",
      "RMSE of Fold 2 is 9.6525013772\n",
      "Training 3 Fold\n",
      "R2 Scored of Fold 3 is 0.581304296484\n",
      "RMSE of Fold 3 is 8.01147092298\n",
      "Training 4 Fold\n",
      "R2 Scored of Fold 4 is 0.651904807622\n",
      "RMSE of Fold 4 is 7.13828311127\n",
      "Training 5 Fold\n",
      "R2 Scored of Fold 5 is 0.558957979002\n",
      "RMSE of Fold 5 is 8.10989214582\n",
      "Start Training\n",
      "Calculating In-Bag R2 Score\n",
      "0.608448568119\n",
      "Calculating Out-Bag R2 Score\n",
      "0.570455905719\n",
      "Calculating In-Bag RMSE\n",
      "7.93306244146\n",
      "Calculating Out-Bag RMSE\n",
      "8.29665455628\n"
     ]
    }
   ],
   "source": [
    "lgb_train,lgb_test,ID = get_lgb_stack_data(params,1200,stack_train,col,stack_train['label'],stack_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Level 3: LassoCV as Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stack_level3_train = pd.DataFrame({'ID':xgb_level2_train['ID'],'xgb':xgb_level2_train['predicted'],'label':xgb_level2_train['label']})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stack_level3_train['lgb'] = lgb_train['predicted']\n",
    "stack_level3_train['gbt'] = gbdt_level2_train['predicted']\n",
    "stack_level3_train['las'] = lasso_level2_train['predicted']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stack_level3_test = pd.DataFrame({'ID':xgb_level2_test['ID'],'xgb':xgb_level2_test['y']})\n",
    "stack_level3_test['lgb'] = lgb_test.y\n",
    "stack_level3_test['gbt'] = gbdt_level2_test['y']\n",
    "stack_level3_test['las'] = lasso_level2_test['y']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "col = ['xgb','lgb','gbt','las']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training 1 Fold\n",
      "R2 Scored of Fold 1 is 0.580873432294\n",
      "RMSE of Fold 1 is 8.54874132174\n",
      "Training 2 Fold\n",
      "R2 Scored of Fold 2 is 0.484079390524\n",
      "RMSE of Fold 2 is 9.62787505907\n",
      "Training 3 Fold\n",
      "R2 Scored of Fold 3 is 0.580037986225\n",
      "RMSE of Fold 3 is 8.02357678963\n",
      "Training 4 Fold\n",
      "R2 Scored of Fold 4 is 0.660281156263\n",
      "RMSE of Fold 4 is 7.0518744975\n",
      "Training 5 Fold\n",
      "R2 Scored of Fold 5 is 0.581570857527\n",
      "RMSE of Fold 5 is 7.89925358479\n",
      "Start Training\n",
      "Calculating In-Bag R2 Score\n",
      "0.575961601362\n",
      "Calculating Out-Bag R2 Score\n",
      "0.577368564567\n",
      "Calculating In-Bag RMSE\n",
      "8.25560788694\n",
      "Calculating Out-Bag RMSE\n",
      "8.23026425055\n"
     ]
    }
   ],
   "source": [
    "TL_Stack_TRAIN,TL_Stack_TEST,ID = get_sklearn_stack_data(Lasso(alpha=0.1),stack_level3_train,col,stack_level3_train['label'],stack_level3_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "save_results(TL_Stack_TRAIN,TL_Stack_TEST,'first_three_level_stacking.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Today: Try to reproduce popular Kernel Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jiashen/env3/lib/python3.5/site-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n",
      "/home/jiashen/env3/lib/python3.5/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 2 iterations, i.e. alpha=6.025e-02, with an active set of 2 regressors, and the smallest cholesky pivot element being 1.490e-08\n",
      "  ConvergenceWarning)\n",
      "/home/jiashen/env3/lib/python3.5/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 3 iterations, i.e. alpha=5.597e-02, with an active set of 3 regressors, and the smallest cholesky pivot element being 1.054e-08\n",
      "  ConvergenceWarning)\n",
      "/home/jiashen/env3/lib/python3.5/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 3 iterations, i.e. alpha=5.597e-02, with an active set of 3 regressors, and the smallest cholesky pivot element being 1.490e-08\n",
      "  ConvergenceWarning)\n",
      "/home/jiashen/env3/lib/python3.5/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 4 iterations, i.e. alpha=4.193e-02, with an active set of 4 regressors, and the smallest cholesky pivot element being 1.490e-08\n",
      "  ConvergenceWarning)\n",
      "/home/jiashen/env3/lib/python3.5/site-packages/sklearn/linear_model/least_angle.py:377: RuntimeWarning: overflow encountered in true_divide\n",
      "  g1 = arrayfuncs.min_pos((C - Cov) / (AA - corr_eq_dir + tiny))\n",
      "/home/jiashen/env3/lib/python3.5/site-packages/sklearn/linear_model/least_angle.py:381: RuntimeWarning: overflow encountered in true_divide\n",
      "  g2 = arrayfuncs.min_pos((C + Cov) / (AA + corr_eq_dir + tiny))\n",
      "/home/jiashen/env3/lib/python3.5/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=2.963e-02, with an active set of 7 regressors, and the smallest cholesky pivot element being 1.490e-08\n",
      "  ConvergenceWarning)\n",
      "/home/jiashen/env3/lib/python3.5/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 8 iterations, i.e. alpha=2.850e-02, with an active set of 8 regressors, and the smallest cholesky pivot element being 1.490e-08\n",
      "  ConvergenceWarning)\n",
      "/home/jiashen/env3/lib/python3.5/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 9 iterations, i.e. alpha=2.734e-02, with an active set of 9 regressors, and the smallest cholesky pivot element being 1.825e-08\n",
      "  ConvergenceWarning)\n",
      "/home/jiashen/env3/lib/python3.5/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 9 iterations, i.e. alpha=2.734e-02, with an active set of 9 regressors, and the smallest cholesky pivot element being 1.490e-08\n",
      "  ConvergenceWarning)\n",
      "/home/jiashen/env3/lib/python3.5/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 11 iterations, i.e. alpha=2.238e-02, with an active set of 11 regressors, and the smallest cholesky pivot element being 1.490e-08\n",
      "  ConvergenceWarning)\n",
      "/home/jiashen/env3/lib/python3.5/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 12 iterations, i.e. alpha=2.127e-02, with an active set of 12 regressors, and the smallest cholesky pivot element being 1.490e-08\n",
      "  ConvergenceWarning)\n",
      "/home/jiashen/env3/lib/python3.5/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 13 iterations, i.e. alpha=2.026e-02, with an active set of 13 regressors, and the smallest cholesky pivot element being 1.490e-08\n",
      "  ConvergenceWarning)\n",
      "/home/jiashen/env3/lib/python3.5/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 13 iterations, i.e. alpha=2.026e-02, with an active set of 13 regressors, and the smallest cholesky pivot element being 2.220e-16\n",
      "  ConvergenceWarning)\n",
      "/home/jiashen/env3/lib/python3.5/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 14 iterations, i.e. alpha=1.871e-02, with an active set of 14 regressors, and the smallest cholesky pivot element being 1.490e-08\n",
      "  ConvergenceWarning)\n",
      "/home/jiashen/env3/lib/python3.5/site-packages/sklearn/linear_model/least_angle.py:334: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 16 iterations, alpha=1.861e-02, previous alpha=1.858e-02, with an active set of 15 regressors.\n",
      "  ConvergenceWarning)\n",
      "/home/jiashen/env3/lib/python3.5/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 4 iterations, i.e. alpha=2.358e-02, with an active set of 4 regressors, and the smallest cholesky pivot element being 1.054e-08\n",
      "  ConvergenceWarning)\n",
      "/home/jiashen/env3/lib/python3.5/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 11 iterations, i.e. alpha=1.488e-02, with an active set of 11 regressors, and the smallest cholesky pivot element being 2.220e-16\n",
      "  ConvergenceWarning)\n",
      "/home/jiashen/env3/lib/python3.5/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 12 iterations, i.e. alpha=1.222e-02, with an active set of 12 regressors, and the smallest cholesky pivot element being 2.220e-16\n",
      "  ConvergenceWarning)\n",
      "/home/jiashen/env3/lib/python3.5/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 14 iterations, i.e. alpha=1.122e-02, with an active set of 14 regressors, and the smallest cholesky pivot element being 1.054e-08\n",
      "  ConvergenceWarning)\n",
      "/home/jiashen/env3/lib/python3.5/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 14 iterations, i.e. alpha=1.122e-02, with an active set of 14 regressors, and the smallest cholesky pivot element being 2.220e-16\n",
      "  ConvergenceWarning)\n",
      "/home/jiashen/env3/lib/python3.5/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 14 iterations, i.e. alpha=1.122e-02, with an active set of 14 regressors, and the smallest cholesky pivot element being 1.825e-08\n",
      "  ConvergenceWarning)\n",
      "/home/jiashen/env3/lib/python3.5/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 17 iterations, i.e. alpha=8.549e-03, with an active set of 17 regressors, and the smallest cholesky pivot element being 1.490e-08\n",
      "  ConvergenceWarning)\n",
      "/home/jiashen/env3/lib/python3.5/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 21 iterations, i.e. alpha=6.575e-03, with an active set of 21 regressors, and the smallest cholesky pivot element being 1.490e-08\n",
      "  ConvergenceWarning)\n",
      "/home/jiashen/env3/lib/python3.5/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 23 iterations, i.e. alpha=6.335e-03, with an active set of 23 regressors, and the smallest cholesky pivot element being 1.490e-08\n",
      "  ConvergenceWarning)\n",
      "/home/jiashen/env3/lib/python3.5/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 32 iterations, i.e. alpha=5.505e-03, with an active set of 30 regressors, and the smallest cholesky pivot element being 2.220e-16\n",
      "  ConvergenceWarning)\n",
      "/home/jiashen/env3/lib/python3.5/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 32 iterations, i.e. alpha=5.505e-03, with an active set of 30 regressors, and the smallest cholesky pivot element being 1.490e-08\n",
      "  ConvergenceWarning)\n",
      "/home/jiashen/env3/lib/python3.5/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 32 iterations, i.e. alpha=5.505e-03, with an active set of 30 regressors, and the smallest cholesky pivot element being 1.054e-08\n",
      "  ConvergenceWarning)\n",
      "/home/jiashen/env3/lib/python3.5/site-packages/sklearn/linear_model/least_angle.py:334: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 33 iterations, alpha=5.479e-03, previous alpha=5.420e-03, with an active set of 30 regressors.\n",
      "  ConvergenceWarning)\n",
      "/home/jiashen/env3/lib/python3.5/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 3 iterations, i.e. alpha=5.688e-02, with an active set of 3 regressors, and the smallest cholesky pivot element being 2.220e-16\n",
      "  ConvergenceWarning)\n",
      "/home/jiashen/env3/lib/python3.5/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 4 iterations, i.e. alpha=2.964e-02, with an active set of 4 regressors, and the smallest cholesky pivot element being 2.220e-16\n",
      "  ConvergenceWarning)\n",
      "/home/jiashen/env3/lib/python3.5/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 4 iterations, i.e. alpha=2.964e-02, with an active set of 4 regressors, and the smallest cholesky pivot element being 1.490e-08\n",
      "  ConvergenceWarning)\n",
      "/home/jiashen/env3/lib/python3.5/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 15 iterations, i.e. alpha=1.454e-02, with an active set of 15 regressors, and the smallest cholesky pivot element being 2.220e-16\n",
      "  ConvergenceWarning)\n",
      "/home/jiashen/env3/lib/python3.5/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 15 iterations, i.e. alpha=1.454e-02, with an active set of 15 regressors, and the smallest cholesky pivot element being 1.490e-08\n",
      "  ConvergenceWarning)\n",
      "/home/jiashen/env3/lib/python3.5/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 15 iterations, i.e. alpha=1.441e-02, with an active set of 15 regressors, and the smallest cholesky pivot element being 1.490e-08\n",
      "  ConvergenceWarning)\n",
      "/home/jiashen/env3/lib/python3.5/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 24 iterations, i.e. alpha=8.665e-03, with an active set of 20 regressors, and the smallest cholesky pivot element being 1.054e-08\n",
      "  ConvergenceWarning)\n",
      "/home/jiashen/env3/lib/python3.5/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 26 iterations, i.e. alpha=8.230e-03, with an active set of 22 regressors, and the smallest cholesky pivot element being 1.490e-08\n",
      "  ConvergenceWarning)\n",
      "/home/jiashen/env3/lib/python3.5/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 26 iterations, i.e. alpha=8.230e-03, with an active set of 22 regressors, and the smallest cholesky pivot element being 1.054e-08\n",
      "  ConvergenceWarning)\n",
      "/home/jiashen/env3/lib/python3.5/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 30 iterations, i.e. alpha=7.253e-03, with an active set of 26 regressors, and the smallest cholesky pivot element being 1.490e-08\n",
      "  ConvergenceWarning)\n",
      "/home/jiashen/env3/lib/python3.5/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 36 iterations, i.e. alpha=5.798e-03, with an active set of 32 regressors, and the smallest cholesky pivot element being 1.054e-08\n",
      "  ConvergenceWarning)\n",
      "/home/jiashen/env3/lib/python3.5/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 54 iterations, i.e. alpha=4.115e-03, with an active set of 48 regressors, and the smallest cholesky pivot element being 2.220e-16\n",
      "  ConvergenceWarning)\n",
      "/home/jiashen/env3/lib/python3.5/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 54 iterations, i.e. alpha=4.115e-03, with an active set of 48 regressors, and the smallest cholesky pivot element being 1.054e-08\n",
      "  ConvergenceWarning)\n",
      "/home/jiashen/env3/lib/python3.5/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 54 iterations, i.e. alpha=4.115e-03, with an active set of 48 regressors, and the smallest cholesky pivot element being 2.107e-08\n",
      "  ConvergenceWarning)\n",
      "/home/jiashen/env3/lib/python3.5/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 54 iterations, i.e. alpha=4.115e-03, with an active set of 48 regressors, and the smallest cholesky pivot element being 1.490e-08\n",
      "  ConvergenceWarning)\n",
      "/home/jiashen/env3/lib/python3.5/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 54 iterations, i.e. alpha=4.111e-03, with an active set of 48 regressors, and the smallest cholesky pivot element being 9.186e-08\n",
      "  ConvergenceWarning)\n",
      "/home/jiashen/env3/lib/python3.5/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 57 iterations, i.e. alpha=3.922e-03, with an active set of 51 regressors, and the smallest cholesky pivot element being 1.054e-08\n",
      "  ConvergenceWarning)\n",
      "/home/jiashen/env3/lib/python3.5/site-packages/sklearn/linear_model/least_angle.py:334: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 63 iterations, alpha=4.241e-03, previous alpha=3.692e-03, with an active set of 56 regressors.\n",
      "  ConvergenceWarning)\n",
      "/home/jiashen/env3/lib/python3.5/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 2 iterations, i.e. alpha=4.811e-02, with an active set of 2 regressors, and the smallest cholesky pivot element being 2.220e-16\n",
      "  ConvergenceWarning)\n",
      "/home/jiashen/env3/lib/python3.5/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 3 iterations, i.e. alpha=4.682e-02, with an active set of 3 regressors, and the smallest cholesky pivot element being 2.220e-16\n",
      "  ConvergenceWarning)\n",
      "/home/jiashen/env3/lib/python3.5/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 4 iterations, i.e. alpha=3.232e-02, with an active set of 4 regressors, and the smallest cholesky pivot element being 2.220e-16\n",
      "  ConvergenceWarning)\n",
      "/home/jiashen/env3/lib/python3.5/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=2.304e-02, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.220e-16\n",
      "  ConvergenceWarning)\n",
      "/home/jiashen/env3/lib/python3.5/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=2.279e-02, with an active set of 7 regressors, and the smallest cholesky pivot element being 2.220e-16\n",
      "  ConvergenceWarning)\n",
      "/home/jiashen/env3/lib/python3.5/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 10 iterations, i.e. alpha=1.721e-02, with an active set of 8 regressors, and the smallest cholesky pivot element being 2.220e-16\n",
      "  ConvergenceWarning)\n",
      "/home/jiashen/env3/lib/python3.5/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 10 iterations, i.e. alpha=1.714e-02, with an active set of 8 regressors, and the smallest cholesky pivot element being 2.220e-16\n",
      "  ConvergenceWarning)\n",
      "/home/jiashen/env3/lib/python3.5/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 15 iterations, i.e. alpha=1.183e-02, with an active set of 13 regressors, and the smallest cholesky pivot element being 4.712e-08\n",
      "  ConvergenceWarning)\n",
      "/home/jiashen/env3/lib/python3.5/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 15 iterations, i.e. alpha=1.183e-02, with an active set of 13 regressors, and the smallest cholesky pivot element being 1.490e-08\n",
      "  ConvergenceWarning)\n",
      "/home/jiashen/env3/lib/python3.5/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 16 iterations, i.e. alpha=1.134e-02, with an active set of 14 regressors, and the smallest cholesky pivot element being 1.490e-08\n",
      "  ConvergenceWarning)\n",
      "/home/jiashen/env3/lib/python3.5/site-packages/sklearn/linear_model/least_angle.py:334: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 18 iterations, alpha=1.098e-02, previous alpha=1.096e-02, with an active set of 15 regressors.\n",
      "  ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jiashen/env3/lib/python3.5/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=8.590e-03, with an active set of 5 regressors, and the smallest cholesky pivot element being 2.220e-16\n",
      "  ConvergenceWarning)\n",
      "/home/jiashen/env3/lib/python3.5/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 10 iterations, i.e. alpha=5.601e-03, with an active set of 10 regressors, and the smallest cholesky pivot element being 2.220e-16\n",
      "  ConvergenceWarning)\n",
      "/home/jiashen/env3/lib/python3.5/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 11 iterations, i.e. alpha=5.191e-03, with an active set of 11 regressors, and the smallest cholesky pivot element being 2.220e-16\n",
      "  ConvergenceWarning)\n",
      "/home/jiashen/env3/lib/python3.5/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 15 iterations, i.e. alpha=4.793e-03, with an active set of 15 regressors, and the smallest cholesky pivot element being 2.220e-16\n",
      "  ConvergenceWarning)\n",
      "/home/jiashen/env3/lib/python3.5/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 16 iterations, i.e. alpha=4.711e-03, with an active set of 16 regressors, and the smallest cholesky pivot element being 2.220e-16\n",
      "  ConvergenceWarning)\n",
      "/home/jiashen/env3/lib/python3.5/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 16 iterations, i.e. alpha=4.691e-03, with an active set of 16 regressors, and the smallest cholesky pivot element being 2.220e-16\n",
      "  ConvergenceWarning)\n",
      "/home/jiashen/env3/lib/python3.5/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 54 iterations, i.e. alpha=2.575e-03, with an active set of 50 regressors, and the smallest cholesky pivot element being 2.220e-16\n",
      "  ConvergenceWarning)\n",
      "/home/jiashen/env3/lib/python3.5/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 55 iterations, i.e. alpha=2.560e-03, with an active set of 51 regressors, and the smallest cholesky pivot element being 2.220e-16\n",
      "  ConvergenceWarning)\n",
      "/home/jiashen/env3/lib/python3.5/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 65 iterations, i.e. alpha=2.390e-03, with an active set of 61 regressors, and the smallest cholesky pivot element being 2.220e-16\n",
      "  ConvergenceWarning)\n",
      "/home/jiashen/env3/lib/python3.5/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 67 iterations, i.e. alpha=2.336e-03, with an active set of 63 regressors, and the smallest cholesky pivot element being 2.220e-16\n",
      "  ConvergenceWarning)\n",
      "/home/jiashen/env3/lib/python3.5/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 73 iterations, i.e. alpha=2.243e-03, with an active set of 69 regressors, and the smallest cholesky pivot element being 1.054e-08\n",
      "  ConvergenceWarning)\n",
      "/home/jiashen/env3/lib/python3.5/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 73 iterations, i.e. alpha=2.243e-03, with an active set of 69 regressors, and the smallest cholesky pivot element being 2.220e-16\n",
      "  ConvergenceWarning)\n",
      "/home/jiashen/env3/lib/python3.5/site-packages/sklearn/linear_model/least_angle.py:334: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 74 iterations, alpha=2.285e-03, previous alpha=2.220e-03, with an active set of 69 regressors.\n",
      "  ConvergenceWarning)\n",
      "/home/jiashen/env3/lib/python3.5/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=7.350e-03, with an active set of 7 regressors, and the smallest cholesky pivot element being 2.220e-16\n",
      "  ConvergenceWarning)\n",
      "/home/jiashen/env3/lib/python3.5/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 8 iterations, i.e. alpha=6.615e-03, with an active set of 8 regressors, and the smallest cholesky pivot element being 2.220e-16\n",
      "  ConvergenceWarning)\n",
      "/home/jiashen/env3/lib/python3.5/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 15 iterations, i.e. alpha=4.974e-03, with an active set of 15 regressors, and the smallest cholesky pivot element being 1.054e-08\n",
      "  ConvergenceWarning)\n",
      "/home/jiashen/env3/lib/python3.5/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 15 iterations, i.e. alpha=4.974e-03, with an active set of 15 regressors, and the smallest cholesky pivot element being 2.220e-16\n",
      "  ConvergenceWarning)\n",
      "/home/jiashen/env3/lib/python3.5/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 25 iterations, i.e. alpha=4.206e-03, with an active set of 25 regressors, and the smallest cholesky pivot element being 1.825e-08\n",
      "  ConvergenceWarning)\n",
      "/home/jiashen/env3/lib/python3.5/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 26 iterations, i.e. alpha=4.115e-03, with an active set of 26 regressors, and the smallest cholesky pivot element being 1.490e-08\n",
      "  ConvergenceWarning)\n",
      "/home/jiashen/env3/lib/python3.5/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 31 iterations, i.e. alpha=3.673e-03, with an active set of 31 regressors, and the smallest cholesky pivot element being 1.490e-08\n",
      "  ConvergenceWarning)\n",
      "/home/jiashen/env3/lib/python3.5/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 35 iterations, i.e. alpha=3.441e-03, with an active set of 35 regressors, and the smallest cholesky pivot element being 1.490e-08\n",
      "  ConvergenceWarning)\n",
      "/home/jiashen/env3/lib/python3.5/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 35 iterations, i.e. alpha=3.441e-03, with an active set of 35 regressors, and the smallest cholesky pivot element being 1.054e-08\n",
      "  ConvergenceWarning)\n",
      "/home/jiashen/env3/lib/python3.5/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 52 iterations, i.e. alpha=2.549e-03, with an active set of 52 regressors, and the smallest cholesky pivot element being 1.054e-08\n",
      "  ConvergenceWarning)\n",
      "/home/jiashen/env3/lib/python3.5/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 55 iterations, i.e. alpha=2.506e-03, with an active set of 55 regressors, and the smallest cholesky pivot element being 1.490e-08\n",
      "  ConvergenceWarning)\n",
      "/home/jiashen/env3/lib/python3.5/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 56 iterations, i.e. alpha=2.503e-03, with an active set of 56 regressors, and the smallest cholesky pivot element being 1.490e-08\n",
      "  ConvergenceWarning)\n",
      "/home/jiashen/env3/lib/python3.5/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 57 iterations, i.e. alpha=2.454e-03, with an active set of 57 regressors, and the smallest cholesky pivot element being 1.054e-08\n",
      "  ConvergenceWarning)\n",
      "/home/jiashen/env3/lib/python3.5/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 57 iterations, i.e. alpha=2.454e-03, with an active set of 57 regressors, and the smallest cholesky pivot element being 1.490e-08\n",
      "  ConvergenceWarning)\n",
      "/home/jiashen/env3/lib/python3.5/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 58 iterations, i.e. alpha=2.417e-03, with an active set of 58 regressors, and the smallest cholesky pivot element being 1.054e-08\n",
      "  ConvergenceWarning)\n",
      "/home/jiashen/env3/lib/python3.5/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 58 iterations, i.e. alpha=2.417e-03, with an active set of 58 regressors, and the smallest cholesky pivot element being 1.490e-08\n",
      "  ConvergenceWarning)\n",
      "/home/jiashen/env3/lib/python3.5/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 62 iterations, i.e. alpha=2.235e-03, with an active set of 62 regressors, and the smallest cholesky pivot element being 2.220e-16\n",
      "  ConvergenceWarning)\n",
      "/home/jiashen/env3/lib/python3.5/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 62 iterations, i.e. alpha=2.235e-03, with an active set of 62 regressors, and the smallest cholesky pivot element being 1.490e-08\n",
      "  ConvergenceWarning)\n",
      "/home/jiashen/env3/lib/python3.5/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 71 iterations, i.e. alpha=2.042e-03, with an active set of 71 regressors, and the smallest cholesky pivot element being 2.220e-16\n",
      "  ConvergenceWarning)\n",
      "/home/jiashen/env3/lib/python3.5/site-packages/sklearn/linear_model/least_angle.py:334: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 77 iterations, alpha=1.924e-03, previous alpha=1.896e-03, with an active set of 76 regressors.\n",
      "  ConvergenceWarning)\n",
      "/home/jiashen/env3/lib/python3.5/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 9 iterations, i.e. alpha=7.281e-03, with an active set of 7 regressors, and the smallest cholesky pivot element being 1.054e-08\n",
      "  ConvergenceWarning)\n",
      "/home/jiashen/env3/lib/python3.5/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 10 iterations, i.e. alpha=7.276e-03, with an active set of 8 regressors, and the smallest cholesky pivot element being 1.490e-08\n",
      "  ConvergenceWarning)\n",
      "/home/jiashen/env3/lib/python3.5/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 13 iterations, i.e. alpha=5.844e-03, with an active set of 11 regressors, and the smallest cholesky pivot element being 1.490e-08\n",
      "  ConvergenceWarning)\n",
      "/home/jiashen/env3/lib/python3.5/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 13 iterations, i.e. alpha=5.844e-03, with an active set of 11 regressors, and the smallest cholesky pivot element being 1.054e-08\n",
      "  ConvergenceWarning)\n",
      "/home/jiashen/env3/lib/python3.5/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 16 iterations, i.e. alpha=5.095e-03, with an active set of 14 regressors, and the smallest cholesky pivot element being 1.054e-08\n",
      "  ConvergenceWarning)\n",
      "/home/jiashen/env3/lib/python3.5/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 16 iterations, i.e. alpha=5.095e-03, with an active set of 14 regressors, and the smallest cholesky pivot element being 1.825e-08\n",
      "  ConvergenceWarning)\n",
      "/home/jiashen/env3/lib/python3.5/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 17 iterations, i.e. alpha=4.992e-03, with an active set of 15 regressors, and the smallest cholesky pivot element being 1.490e-08\n",
      "  ConvergenceWarning)\n",
      "/home/jiashen/env3/lib/python3.5/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 20 iterations, i.e. alpha=4.462e-03, with an active set of 18 regressors, and the smallest cholesky pivot element being 1.490e-08\n",
      "  ConvergenceWarning)\n",
      "/home/jiashen/env3/lib/python3.5/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 27 iterations, i.e. alpha=4.266e-03, with an active set of 25 regressors, and the smallest cholesky pivot element being 1.054e-08\n",
      "  ConvergenceWarning)\n",
      "/home/jiashen/env3/lib/python3.5/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 32 iterations, i.e. alpha=4.070e-03, with an active set of 30 regressors, and the smallest cholesky pivot element being 1.054e-08\n",
      "  ConvergenceWarning)\n",
      "/home/jiashen/env3/lib/python3.5/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 34 iterations, i.e. alpha=3.933e-03, with an active set of 32 regressors, and the smallest cholesky pivot element being 1.054e-08\n",
      "  ConvergenceWarning)\n",
      "/home/jiashen/env3/lib/python3.5/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 39 iterations, i.e. alpha=3.436e-03, with an active set of 37 regressors, and the smallest cholesky pivot element being 1.054e-08\n",
      "  ConvergenceWarning)\n",
      "/home/jiashen/env3/lib/python3.5/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 50 iterations, i.e. alpha=2.860e-03, with an active set of 48 regressors, and the smallest cholesky pivot element being 2.220e-16\n",
      "  ConvergenceWarning)\n",
      "/home/jiashen/env3/lib/python3.5/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 50 iterations, i.e. alpha=2.860e-03, with an active set of 48 regressors, and the smallest cholesky pivot element being 1.490e-08\n",
      "  ConvergenceWarning)\n",
      "/home/jiashen/env3/lib/python3.5/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 55 iterations, i.e. alpha=2.751e-03, with an active set of 53 regressors, and the smallest cholesky pivot element being 1.490e-08\n",
      "  ConvergenceWarning)\n",
      "/home/jiashen/env3/lib/python3.5/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 55 iterations, i.e. alpha=2.751e-03, with an active set of 53 regressors, and the smallest cholesky pivot element being 1.054e-08\n",
      "  ConvergenceWarning)\n",
      "/home/jiashen/env3/lib/python3.5/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 58 iterations, i.e. alpha=2.668e-03, with an active set of 56 regressors, and the smallest cholesky pivot element being 1.490e-08\n",
      "  ConvergenceWarning)\n",
      "/home/jiashen/env3/lib/python3.5/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 62 iterations, i.e. alpha=2.455e-03, with an active set of 60 regressors, and the smallest cholesky pivot element being 1.054e-08\n",
      "  ConvergenceWarning)\n",
      "/home/jiashen/env3/lib/python3.5/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 62 iterations, i.e. alpha=2.455e-03, with an active set of 60 regressors, and the smallest cholesky pivot element being 1.490e-08\n",
      "  ConvergenceWarning)\n",
      "/home/jiashen/env3/lib/python3.5/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 62 iterations, i.e. alpha=2.452e-03, with an active set of 60 regressors, and the smallest cholesky pivot element being 1.054e-08\n",
      "  ConvergenceWarning)\n",
      "/home/jiashen/env3/lib/python3.5/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 68 iterations, i.e. alpha=2.238e-03, with an active set of 66 regressors, and the smallest cholesky pivot element being 1.825e-08\n",
      "  ConvergenceWarning)\n",
      "/home/jiashen/env3/lib/python3.5/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 83 iterations, i.e. alpha=1.966e-03, with an active set of 79 regressors, and the smallest cholesky pivot element being 1.054e-08\n",
      "  ConvergenceWarning)\n",
      "/home/jiashen/env3/lib/python3.5/site-packages/sklearn/linear_model/least_angle.py:334: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 90 iterations, alpha=1.993e-03, previous alpha=1.885e-03, with an active set of 83 regressors.\n",
      "  ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R2 score on train data:\n",
      "0.658927980335\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load in \n",
    "\n",
    "import numpy as np\n",
    "from sklearn.base import BaseEstimator,TransformerMixin, ClassifierMixin\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import xgboost as xgb\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "from sklearn.linear_model import ElasticNetCV, LassoLarsCV\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.pipeline import make_pipeline, make_union\n",
    "from sklearn.utils import check_array\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.random_projection import GaussianRandomProjection\n",
    "from sklearn.random_projection import SparseRandomProjection\n",
    "from sklearn.decomposition import PCA, FastICA\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "\n",
    "\n",
    "class StackingEstimator(BaseEstimator, TransformerMixin):\n",
    "    \n",
    "    def __init__(self, estimator):\n",
    "        self.estimator = estimator\n",
    "\n",
    "    def fit(self, X, y=None, **fit_params):\n",
    "        self.estimator.fit(X, y, **fit_params)\n",
    "        return self\n",
    "    def transform(self, X):\n",
    "        X = check_array(X)\n",
    "        X_transformed = np.copy(X)\n",
    "        # add class probabilities as a synthetic feature\n",
    "        if issubclass(self.estimator.__class__, ClassifierMixin) and hasattr(self.estimator, 'predict_proba'):\n",
    "            X_transformed = np.hstack((self.estimator.predict_proba(X), X))\n",
    "\n",
    "        # add class prodiction as a synthetic feature\n",
    "        X_transformed = np.hstack((np.reshape(self.estimator.predict(X), (-1, 1)), X_transformed))\n",
    "\n",
    "        return X_transformed\n",
    "\n",
    "\n",
    "train = pd.read_csv('data/train.csv')\n",
    "test = pd.read_csv('data/test.csv')\n",
    "\n",
    "for c in train.columns:\n",
    "    if train[c].dtype == 'object':\n",
    "        lbl = LabelEncoder()\n",
    "        lbl.fit(list(train[c].values) + list(test[c].values))\n",
    "        train[c] = lbl.transform(list(train[c].values))\n",
    "        test[c] = lbl.transform(list(test[c].values))\n",
    "\n",
    "\n",
    "\n",
    "n_comp = 12\n",
    "\n",
    "# tSVD\n",
    "tsvd = TruncatedSVD(n_components=n_comp, random_state=420)\n",
    "tsvd_results_train = tsvd.fit_transform(train.drop([\"y\"], axis=1))\n",
    "tsvd_results_test = tsvd.transform(test)\n",
    "\n",
    "# PCA\n",
    "pca = PCA(n_components=n_comp, random_state=420)\n",
    "pca2_results_train = pca.fit_transform(train.drop([\"y\"], axis=1))\n",
    "pca2_results_test = pca.transform(test)\n",
    "\n",
    "# ICA\n",
    "ica = FastICA(n_components=n_comp, random_state=420)\n",
    "ica2_results_train = ica.fit_transform(train.drop([\"y\"], axis=1))\n",
    "ica2_results_test = ica.transform(test)\n",
    "\n",
    "# GRP\n",
    "grp = GaussianRandomProjection(n_components=n_comp, eps=0.1, random_state=420)\n",
    "grp_results_train = grp.fit_transform(train.drop([\"y\"], axis=1))\n",
    "grp_results_test = grp.transform(test)\n",
    "\n",
    "# SRP\n",
    "srp = SparseRandomProjection(n_components=n_comp, dense_output=True, random_state=420)\n",
    "srp_results_train = srp.fit_transform(train.drop([\"y\"], axis=1))\n",
    "srp_results_test = srp.transform(test)\n",
    "\n",
    "#save columns list before adding the decomposition components\n",
    "\n",
    "usable_columns = list(set(train.columns) - set(['y']))\n",
    "\n",
    "# Append decomposition components to datasets\n",
    "for i in range(1, n_comp + 1):\n",
    "    train['pca_' + str(i)] = pca2_results_train[:, i - 1]\n",
    "    test['pca_' + str(i)] = pca2_results_test[:, i - 1]\n",
    "\n",
    "    train['ica_' + str(i)] = ica2_results_train[:, i - 1]\n",
    "    test['ica_' + str(i)] = ica2_results_test[:, i - 1]\n",
    "\n",
    "    train['tsvd_' + str(i)] = tsvd_results_train[:, i - 1]\n",
    "    test['tsvd_' + str(i)] = tsvd_results_test[:, i - 1]\n",
    "\n",
    "    train['grp_' + str(i)] = grp_results_train[:, i - 1]\n",
    "    test['grp_' + str(i)] = grp_results_test[:, i - 1]\n",
    "\n",
    "    train['srp_' + str(i)] = srp_results_train[:, i - 1]\n",
    "    test['srp_' + str(i)] = srp_results_test[:, i - 1]\n",
    "\n",
    "#usable_columns = list(set(train.columns) - set(['y']))\n",
    "\n",
    "y_train = train['y'].values\n",
    "y_mean = np.mean(y_train)\n",
    "id_test = test['ID'].values\n",
    "#finaltrainset and finaltestset are data to be used only the stacked model (does not contain PCA, SVD... arrays) \n",
    "finaltrainset = train[usable_columns].values\n",
    "finaltestset = test[usable_columns].values\n",
    "\n",
    "\n",
    "'''Train the xgb model then predict the test data'''\n",
    "\n",
    "xgb_params = {\n",
    "    'n_trees': 520, \n",
    "    'eta': 0.0045,\n",
    "    'max_depth': 4,\n",
    "    'subsample': 0.93,\n",
    "    'objective': 'reg:linear',\n",
    "    'eval_metric': 'rmse',\n",
    "    'base_score': y_mean, # base prediction = mean(target)\n",
    "    'silent': 1\n",
    "}\n",
    "# NOTE: Make sure that the class is labeled 'class' in the data file\n",
    "\n",
    "dtrain = xgb.DMatrix(train.drop('y', axis=1), y_train)\n",
    "dtest = xgb.DMatrix(test)\n",
    "\n",
    "num_boost_rounds = 1250\n",
    "# train model\n",
    "model = xgb.train(dict(xgb_params, silent=0), dtrain, num_boost_round=num_boost_rounds)\n",
    "y_pred = model.predict(dtest)\n",
    "\n",
    "'''Train the stacked models then predict the test data'''\n",
    "\n",
    "stacked_pipeline = make_pipeline(\n",
    "    StackingEstimator(estimator=LassoLarsCV(normalize=True)),\n",
    "    StackingEstimator(estimator=GradientBoostingRegressor(learning_rate=0.001, loss=\"huber\", max_depth=3, max_features=0.55, min_samples_leaf=18, min_samples_split=14, subsample=0.7)),\n",
    "    LassoLarsCV()\n",
    "\n",
    ")\n",
    "\n",
    "\n",
    "stacked_pipeline.fit(finaltrainset, y_train)\n",
    "results = stacked_pipeline.predict(finaltestset)\n",
    "\n",
    "'''R2 Score on the entire Train data when averaging'''\n",
    "\n",
    "print('R2 score on train data:')\n",
    "print(r2_score(y_train,stacked_pipeline.predict(finaltrainset)*0.2855 + model.predict(dtrain)*0.7145))\n",
    "\n",
    "'''Average the preditionon test data  of both models then save it on a csv file'''\n",
    "\n",
    "sub = pd.DataFrame()\n",
    "sub['ID'] = id_test\n",
    "sub['y'] = y_pred*0.75 + results*0.25\n",
    "sub.to_csv('submission/Repro_Ker_2.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "R2 score on train data:\n",
    "0.658887674983\n",
    "\n",
    "'Average the preditionon test data  of both models then save it on a csv file'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
